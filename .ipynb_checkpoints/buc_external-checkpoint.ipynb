{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d411c0-5272-450b-9f1c-d9a080e12d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b1dc438-898f-4224-b56d-c504f2575bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of    Country  Year Sex\n",
      "0  Albania  2009   m\n",
      "1  Albania  2009   m\n",
      "2  Albania  2010   m\n",
      "3  Albania  2011   m\n",
      "4  Albania  2011   f\n",
      "5  Albania  2012   f\n",
      "6   Russia  2010   m\n",
      "7   Russia  2012   f\n",
      "8    India  2009   f\n",
      "9    India  2010   f>\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "##DataFrame for unit testing\n",
    "country_name_ls = ['Albania'] *6 + ['Russia']*2 + ['India']*2\n",
    "year_ls = ['2009', '2009','2010', '2011', '2011', '2012', '2010', '2012', '2009', '2010']\n",
    "sex_ls = ['m']*4 + ['f']*2 + ['m'] + ['f']*3\n",
    "test_df = pd.DataFrame()\n",
    "test_df['Country'] = country_name_ls\n",
    "test_df['Year'] = year_ls\n",
    "test_df['Sex'] = sex_ls\n",
    "print(test_df.info)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef756190-8722-4f77-8687-871402720c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess_df:\n",
    "  '''\n",
    "  Class to preprocess DataFrame\n",
    "  '''\n",
    "  def encode_attributes(self, input_df, column_indices):\n",
    "    transformed_dicts_ls = []\n",
    "    transformed_df = input_df.copy(deep = True)\n",
    "    column_names = transformed_df.columns.tolist()\n",
    "    print(f\"{column_names = }\")\n",
    "    for col_iter in column_indices:\n",
    "      temp_dict = {}\n",
    "      temp_key = 0\n",
    "      temp_ls = []\n",
    "      column_name = column_names[col_iter]\n",
    "      for col in transformed_df.iloc[:,col_iter].tolist():\n",
    "        if col not in [*temp_dict.keys()]:\n",
    "          temp_dict[col] = temp_key\n",
    "          temp_key += 1\n",
    "        temp_ls.append(temp_dict[col])\n",
    "      dict_inv = {v:k for k,v in temp_dict.items()}\n",
    "      transformed_dicts_ls.append(dict_inv)\n",
    "      transformed_df[column_name] = temp_ls\n",
    "    return transformed_df, transformed_dicts_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d632684-f8fa-48f0-bd9c-82c5534a2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUC implementation\n",
    "class buc:\n",
    "    '''\n",
    "    Class for implementing BUC\n",
    "    '''\n",
    "    def __init__(self, df, column_enc_dicts_ls, minsup):\n",
    "        self.numDims = df.shape[1]\n",
    "        self.cardinality = []\n",
    "        self.minsup = minsup\n",
    "        self.output_df = None\n",
    "        self.datacounts = [[]] * df.shape[1]\n",
    "        self.attribute_ls = [\"*\"] * df.shape[1]\n",
    "        self.debug_counter = 0\n",
    "        self.output_dict = {}\n",
    "        self.column_enc_dicts_ls = column_enc_dicts_ls\n",
    "        self.file_path = './dfs/iter_run_'\n",
    "\n",
    "    def counting_sort(self, array_a, df_idx_ls):\n",
    "      '''\n",
    "      Inputs \n",
    "      array_a: List to be sorted\n",
    "      df_idx_ls: Index list corresponding to the array_a. For example: DataFrame indices corresponding to array_a.\n",
    "      Output\n",
    "      idx_ls: Order in which df_idx_ls should be arranged so that array_a is in the sorted order.\n",
    "      '''\n",
    "      array_c = [0]*(max(array_a) + 1)\n",
    "      idx_ls = [-1] * (len(array_a))\n",
    "\n",
    "      # print(f\"{array_a = }\")\n",
    "      # print(f\"{array_c = }\")\n",
    "      for i in range(0, len(array_a)):\n",
    "        array_c[array_a[i]] += 1\n",
    "\n",
    "      for i in range(0, len(array_c) - 1):\n",
    "        array_c[i+1] = array_c[i] + array_c[i+1]\n",
    "\n",
    "      for i in range(len(array_a) - 1, -1, -1):\n",
    "        array_c[array_a[i]] = array_c[array_a[i]] - 1\n",
    "        idx = array_c[array_a[i]]\n",
    "        idx_ls[idx] = df_idx_ls[i]\n",
    "\n",
    "      # idx_ls = [i + min_idx for i in idx_ls]\n",
    "      # print(f\"{array_a = }\")\n",
    "      # print(f\"{idx_ls = }\")\n",
    "      return idx_ls\n",
    "\n",
    "\n",
    "    def partition(self, iteration_num, num_splits, d, bigc):\n",
    "        '''\n",
    "        Implements partitioning logic i.e sorts the input dataframe and populates self.datacounts\n",
    "        Inputs:\n",
    "        input_df: Input DataFrame\n",
    "        d: column number based on which sorting is performed\n",
    "        Output:\n",
    "        input_df: DataFrame which is sorted according to the specified column\n",
    "        '''\n",
    "        #Read the entire dataframe and concatenate everything\n",
    "        #################################################ONLY FOR TESTING: REPLACE THIS WITH EXTERNAL MERGE SORT########################################\n",
    "        # print(f\"Inside partition\")\n",
    "        # print(f\"{iteration_num = }\")\n",
    "        # print(f\"{num_splits = }\")\n",
    "        ################################################################################################################################################\n",
    "        df = pd.DataFrame()\n",
    "        for i_iter in range(0, num_splits):\n",
    "            temp_df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "            df = pd.concat([df, temp_df])\n",
    "            \n",
    "        input_df = df\n",
    "        #Sorting the dataframe\n",
    "        temp_counter_dict = {}\n",
    "        sorted_idx = self.counting_sort(input_df.iloc[:,d].tolist(), input_df.index.tolist())\n",
    "        input_df = input_df.reindex(sorted_idx)\n",
    "        #Populating self.datacounts\n",
    "        for attribute in input_df.iloc[:,d].tolist():\n",
    "            temp_counter_dict[attribute] = temp_counter_dict.get(attribute, 0) + 1\n",
    "        self.datacounts[d] = [*temp_counter_dict.values()]\n",
    "        \n",
    "        #Write the sorted files back to the disk\n",
    "        split_df = np.array_split(input_df, num_splits)\n",
    "        for i_iter, df in enumerate(split_df):\n",
    "            df.to_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "        return None        \n",
    "        \n",
    "    def compute_aggregate(self, iteration_num, num_splits):\n",
    "        count = 0\n",
    "        for i_iter in range(0,num_splits):\n",
    "            df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "            count += df.shape[0]\n",
    "        return count\n",
    "\n",
    "    def find_bigc(self, iteration_num, num_splits, d): \n",
    "        '''\n",
    "        Bigc refers to the cardinality of the dth attribute in the dataframe\n",
    "        '''\n",
    "        computed_values = []\n",
    "        bigc = 0\n",
    "        for i_iter in range(0, num_splits):\n",
    "            df = pd.read_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "            for attribute_name in df.iloc[:,d].unique().tolist():\n",
    "                if attribute_name not in computed_values:\n",
    "                    computed_values.append(attribute_name)\n",
    "                    bigc += 1\n",
    "        return bigc\n",
    "\n",
    "    def split_input(self, slice_range, iteration_num):\n",
    "        num_splits = 2\n",
    "        print(f\"{self.file_path}{iteration_num-1}0.pkl\")\n",
    "        if not os.path.exists(f\"{self.file_path}{iteration_num-1}0.pkl\"):\n",
    "            #Dimension = 0\n",
    "            input_df = transformed_df\n",
    "            split_df = np.array_split(input_df, num_splits)\n",
    "            for i_iter, df in enumerate(split_df):\n",
    "                df.to_pickle(f\"{self.file_path}{iteration_num}{i_iter}.pkl\")\n",
    "        else:\n",
    "            #Read the correct blocks from iteration-1 and write them\n",
    "        return num_splits\n",
    "    \n",
    "    def buc_implementation(self, slice_range, dim, iteration_num):\n",
    "        '''\n",
    "        Function to implement BUC as indicated in the original paper. \n",
    "        Populates self.output_dict which is the output dictionary.\n",
    "        NOTE:All the variable names are exactly as indicated in the original paper.\n",
    "        Input\n",
    "        input: Input DataFrame\n",
    "        dim: Starting column for performing aggregation\n",
    "        '''\n",
    "        self.debug_counter += 1\n",
    "        num_splits = self.split_input(slice_range, iteration_num)\n",
    "        if self.debug_counter == 3:\n",
    "            return {}\n",
    "        # print(f\"iter: {self.debug_counter - 1}\")\n",
    "        # print(f\"Aggregate: {input.shape[0]}\")\n",
    "        if tuple(self.attribute_ls) in [*self.output_dict.keys()]:\n",
    "          print(f\"Error!!\")\n",
    "        aggregate = self.compute_aggregate(iteration_num, num_splits)\n",
    "        # print(f\"{aggregate = }\")\n",
    "        self.output_dict[tuple(self.attribute_ls)] = aggregate\n",
    "        # print(f\"{self.output_dict =}\")\n",
    "        # if self.debug_counter == 10:\n",
    "            # return \n",
    "        # print(f\"{dim = }\")\n",
    "        for d in range(dim, self.numDims,1):\n",
    "            # bigc = input.iloc[:,d].nunique()\n",
    "            bigc = self.find_bigc(iteration_num, num_splits, d)\n",
    "            # print(f\"{d = }\")\n",
    "            # print(f\"{bigc = }\")\n",
    "            # print(f\"{d= }, {bigc=}\")\n",
    "            # print(f\"Input before partitioning: {input}\")\n",
    "            self.partition(iteration_num, num_splits, d, bigc)\n",
    "            # print(f\"After partition\")\n",
    "            # return {}\n",
    "            # print(f\"Input after partitioning on {d}: {input}\")\n",
    "            # print(f\"{self.datacounts = }\")\n",
    "            k = 0\n",
    "            for i in range(0, bigc, 1):\n",
    "                # print(f\"################Inside i loop######################\")\n",
    "                # print(f\"{input = }\")\n",
    "                # print(f\"{d = }, {i = }\")\n",
    "                # print(f\"{self.datacounts = }\")\n",
    "                smallc = self.datacounts[d][i]\n",
    "                # print(f\"{smallc = }\")\n",
    "                if smallc >= self.minsup:\n",
    "                    # print(f\"**********************Inside if condition***********************\")\n",
    "                    # print(f\"k, d: {k,d}\")\n",
    "                    # print(f\"Attribute: {input.iloc[k,d]}\")\n",
    "                    # print(f\"{transformed_dicts[d] = }\")\n",
    "                    # input = transformed_df\n",
    "                    input = transformed_df\n",
    "                    self.attribute_ls[d] = self.column_enc_dicts_ls[d][input.iloc[k,d]]\n",
    "                    del input\n",
    "                    # print(f\"{self.attribute_ls = }\")\n",
    "                    # self.buc_implementation(input.iloc[k:k+smallc,:], dim=d+1, iteration_num=iteration_num+1)\n",
    "                    self.buc_implementation(slice_range = [k,k+smallc], dim=d+1, iteration_num=iteration_num+1)\n",
    "                    if self.debug_counter == 3:\n",
    "                        return {}\n",
    "                    # print(f\"d inside if condition: {d}\")\n",
    "                    # print(f\"******************************************************************\")\n",
    "                k += smallc\n",
    "            # print(f\"#################################################################\")\n",
    "            # print(f\"ALL:\")\n",
    "            self.attribute_ls[d] = \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2f82375-1a6e-4c4f-8938-e9eb7d9b9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names = ['Country', 'Year', 'Sex']\n",
      "./dfs/iter_run_-10.pkl\n",
      "./dfs/iter_run_00.pkl\n",
      "Inside path exists\n",
      "./dfs/iter_run_10.pkl\n",
      "Inside path exists\n",
      "output_dict = {('*', '*', '*'): 10, ('Albania', '*', '*'): 10}\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "minsup = 1\n",
    "input_df = test_df\n",
    "# print(f\"input_df: {input_df}\")\n",
    "preprocess_obj = preprocess_df()\n",
    "transformed_df, column_enc_dicts_ls = preprocess_obj.encode_attributes(input_df, [*range(0,input_df.shape[1])]) #NOTE: This should be modified as required\n",
    "# print(f\"transformed_df: {transformed_df}\")\n",
    "# print(f\"column_enc_dicts_ls: {column_enc_dicts_ls}\")\n",
    "buc_obj = buc(transformed_df, column_enc_dicts_ls, minsup)\n",
    "# buc_obj.buc_implementation(transformed_df, 0, 0)\n",
    "buc_obj.buc_implementation([0,transformed_df.shape[0]], 0, 0)\n",
    "\n",
    "output_dict = buc_obj.output_dict\n",
    "print(f\"{output_dict = }\")\n",
    "# output_df = pd.DataFrame(columns=input_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91ab0272-d725-4306-ab17-39738e300610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Year  Sex\n",
      "0        0     0    0\n",
      "1        0     0    0\n",
      "2        0     1    0\n",
      "3        0     2    0\n",
      "4        0     2    1\n",
      "   Country  Year  Sex\n",
      "5        0     3    1\n",
      "6        1     1    0\n",
      "7        1     3    1\n",
      "8        2     0    1\n",
      "9        2     1    1\n"
     ]
    }
   ],
   "source": [
    "for i_iter in range(0,2):\n",
    "    df = pd.read_pickle(f\"./dfs/iter_run_0{i_iter}.pkl\")\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
